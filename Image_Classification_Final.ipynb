{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "17JlqO_tplsCBOQWznh7QU6aFfyZpCpE0",
      "authorship_tag": "ABX9TyPopwSk9dk+VL5CqiBc1r00",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tedjung13/KAIRI_intern_CNN/blob/main/Image_Classification_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import everything needed"
      ],
      "metadata": {
        "id": "yiJ5eNG4PvoX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "knRkTGmiPkgn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import shutil\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import from Google Drive (need to download set into My Drive, might take a while). Also, put the train folder (not train_folder) separately from open (the zip file) and put it in My Drive."
      ],
      "metadata": {
        "id": "7m1iFWEmPuDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8AhGSf4QKrp",
        "outputId": "c8b3a323-6bda-4518-a4ad-41b39a17f6cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use GPU if possible"
      ],
      "metadata": {
        "id": "pLdvT1M7P8Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0OzYEuLP4yZ",
        "outputId": "ea0fff99-2236-4e2d-85ce-47f12e1da4b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a path to the folder (that's in Google Drive already). If it's in a different directory, make sure to reflect that change after MyDrive.\n",
        "\n"
      ],
      "metadata": {
        "id": "J-ereWPqFSxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/train'\n",
        "os.chdir(folder_path)"
      ],
      "metadata": {
        "id": "VkLvY0cP2GxX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and read the CSVs: the print should be a list of all the labels."
      ],
      "metadata": {
        "id": "6Ziy4SA1GzF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSVs\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/train_df.csv')\n",
        "label_list = train_df['label'].unique().tolist()\n",
        "print(f'There are {len(label_list)} classes.')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/test_df.csv')\n",
        "test_list = test_df['file_name']\n",
        "print(f'There are {len(test_list)} files in the testing dataset.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrzjWkVwRGUr",
        "outputId": "8c6ce06f-a691-4c68-917f-d422c4278ce6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 88 classes.\n",
            "There are 2154 files in the testing dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a new folder called train_folder where there is a folder of each label."
      ],
      "metadata": {
        "id": "scBG52O_G5Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create folders for each label\n",
        "def create_folder(directory):\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "        else:\n",
        "          print('Already made')\n",
        "    except OSError as error:\n",
        "        print(f'Error: Creating directory {directory}. Error: {error}')\n",
        "\n",
        "for label in label_list:\n",
        "    create_folder(f'/content/drive/MyDrive/train_folder/{label}')\n"
      ],
      "metadata": {
        "id": "U3FvDbaiSTcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7420d2a-a0ac-4f48-91dd-cbab2bfa08af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n",
            "Already made\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the index and the label, move each image to the corresponding folder. For example, 10000.png's label is transistor-good, so the image will be found in the transistor-good folder."
      ],
      "metadata": {
        "id": "S1SfZclJHI8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move files to corresponding label folders\n",
        "train_files = os.listdir('/content/drive/MyDrive/train/')\n",
        "\n",
        "for file in train_files:\n",
        "    if file.endswith(\".png\"):\n",
        "        label = train_df.loc[train_df[\"file_name\"] == file][\"label\"].values[0]\n",
        "        file_source = f'/content/drive/MyDrive/train/{file}'\n",
        "        file_destination = f'/content/drive/MyDrive/train_folder/{label}/'\n",
        "        shutil.move(file_source, file_destination)\n",
        "\n",
        "# Verify files are correctly moved\n",
        "for label in label_list:\n",
        "    label_path = f'/content/drive/MyDrive/train_folder/{label}'\n",
        "    if not os.listdir(label_path):\n",
        "        print(f'No files were found in directory: {label_path}')"
      ],
      "metadata": {
        "id": "euTg5f4GSdMn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the mean and standard deviations to normalize the datasets in the next step. Normalizing is done so all the data is on the same scale. For running time reasons, I resized the data to 256 x 256 instead of 1024 x 1024, but the numbers ended up being about the same. For time-saving purposes, I think you can skip this step and use the numbers I got (this code took 15+ minutes to run), but feel free to run it if you like."
      ],
      "metadata": {
        "id": "wTHkMPwKHtYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_std(data_dir, batch_size = 16):\n",
        "   transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()]) #Might want to change back to 1024 x 1024\n",
        "   dataset = ImageFolder(data_dir, transform=transform)\n",
        "   data_loader = DataLoader(dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "   mean = 0.\n",
        "   std = 0.\n",
        "   nb_samples = 0.\n",
        "   for inputs, _ in data_loader:\n",
        "       batch_samples = inputs.size(0)\n",
        "       inputs = inputs.view(batch_samples, inputs.size(1), -1)\n",
        "       mean += inputs.mean(2).sum(0)\n",
        "       std += inputs.std(2).sum(0)\n",
        "       nb_samples += batch_samples\n",
        "       print(f'{nb_samples} / 4277')\n",
        "   mean /= nb_samples\n",
        "   std /= nb_samples\n",
        "\n",
        "   print(\"Mean:\", mean)\n",
        "   print(\"Standard Deviation:\", std)\n",
        "data_dir = '/content/drive/MyDrive/train_folder'\n",
        "get_mean_std(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEzAikbDHOjl",
        "outputId": "6849864f-640b-4fd7-c904-cbd16b77c8c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16.0\n",
            "32.0\n",
            "48.0\n",
            "64.0\n",
            "80.0\n",
            "96.0\n",
            "112.0\n",
            "128.0\n",
            "144.0\n",
            "160.0\n",
            "176.0\n",
            "192.0\n",
            "208.0\n",
            "224.0\n",
            "240.0\n",
            "256.0\n",
            "272.0\n",
            "288.0\n",
            "304.0\n",
            "320.0\n",
            "336.0\n",
            "352.0\n",
            "368.0\n",
            "384.0\n",
            "400.0\n",
            "416.0\n",
            "432.0\n",
            "448.0\n",
            "464.0\n",
            "480.0\n",
            "496.0\n",
            "512.0\n",
            "528.0\n",
            "544.0\n",
            "560.0\n",
            "576.0\n",
            "592.0\n",
            "608.0\n",
            "624.0\n",
            "640.0\n",
            "656.0\n",
            "672.0\n",
            "688.0\n",
            "704.0\n",
            "720.0\n",
            "736.0\n",
            "752.0\n",
            "768.0\n",
            "784.0\n",
            "800.0\n",
            "816.0\n",
            "832.0\n",
            "848.0\n",
            "864.0\n",
            "880.0\n",
            "896.0\n",
            "912.0\n",
            "928.0\n",
            "944.0\n",
            "960.0\n",
            "976.0\n",
            "992.0\n",
            "1008.0\n",
            "1024.0\n",
            "1040.0\n",
            "1056.0\n",
            "1072.0\n",
            "1088.0\n",
            "1104.0\n",
            "1120.0\n",
            "1136.0\n",
            "1152.0\n",
            "1168.0\n",
            "1184.0\n",
            "1200.0\n",
            "1216.0\n",
            "1232.0\n",
            "1248.0\n",
            "1264.0\n",
            "1280.0\n",
            "1296.0\n",
            "1312.0\n",
            "1328.0\n",
            "1344.0\n",
            "1360.0\n",
            "1376.0\n",
            "1392.0\n",
            "1408.0\n",
            "1424.0\n",
            "1440.0\n",
            "1456.0\n",
            "1472.0\n",
            "1488.0\n",
            "1504.0\n",
            "1520.0\n",
            "1536.0\n",
            "1552.0\n",
            "1568.0\n",
            "1584.0\n",
            "1600.0\n",
            "1616.0\n",
            "1632.0\n",
            "1648.0\n",
            "1664.0\n",
            "1680.0\n",
            "1696.0\n",
            "1712.0\n",
            "1728.0\n",
            "1744.0\n",
            "1760.0\n",
            "1776.0\n",
            "1792.0\n",
            "1808.0\n",
            "1824.0\n",
            "1840.0\n",
            "1856.0\n",
            "1872.0\n",
            "1888.0\n",
            "1904.0\n",
            "1920.0\n",
            "1936.0\n",
            "1952.0\n",
            "1968.0\n",
            "1984.0\n",
            "2000.0\n",
            "2016.0\n",
            "2032.0\n",
            "2048.0\n",
            "2064.0\n",
            "2080.0\n",
            "2096.0\n",
            "2112.0\n",
            "2128.0\n",
            "2144.0\n",
            "2160.0\n",
            "2176.0\n",
            "2192.0\n",
            "2208.0\n",
            "2224.0\n",
            "2240.0\n",
            "2256.0\n",
            "2272.0\n",
            "2288.0\n",
            "2304.0\n",
            "2320.0\n",
            "2336.0\n",
            "2352.0\n",
            "2368.0\n",
            "2384.0\n",
            "2400.0\n",
            "2416.0\n",
            "2432.0\n",
            "2448.0\n",
            "2464.0\n",
            "2480.0\n",
            "2496.0\n",
            "2512.0\n",
            "2528.0\n",
            "2544.0\n",
            "2560.0\n",
            "2576.0\n",
            "2592.0\n",
            "2608.0\n",
            "2624.0\n",
            "2640.0\n",
            "2656.0\n",
            "2672.0\n",
            "2688.0\n",
            "2704.0\n",
            "2720.0\n",
            "2736.0\n",
            "2752.0\n",
            "2768.0\n",
            "2784.0\n",
            "2800.0\n",
            "2816.0\n",
            "2832.0\n",
            "2848.0\n",
            "2864.0\n",
            "2880.0\n",
            "2896.0\n",
            "2912.0\n",
            "2928.0\n",
            "2944.0\n",
            "2960.0\n",
            "2976.0\n",
            "2992.0\n",
            "3008.0\n",
            "3024.0\n",
            "3040.0\n",
            "3056.0\n",
            "3072.0\n",
            "3088.0\n",
            "3104.0\n",
            "3120.0\n",
            "3136.0\n",
            "3152.0\n",
            "3168.0\n",
            "3184.0\n",
            "3200.0\n",
            "3216.0\n",
            "3232.0\n",
            "3248.0\n",
            "3264.0\n",
            "3280.0\n",
            "3296.0\n",
            "3312.0\n",
            "3328.0\n",
            "3344.0\n",
            "3360.0\n",
            "3376.0\n",
            "3392.0\n",
            "3408.0\n",
            "3424.0\n",
            "3440.0\n",
            "3456.0\n",
            "3472.0\n",
            "3488.0\n",
            "3504.0\n",
            "3520.0\n",
            "3536.0\n",
            "3552.0\n",
            "3568.0\n",
            "3584.0\n",
            "3600.0\n",
            "3616.0\n",
            "3632.0\n",
            "3648.0\n",
            "3664.0\n",
            "3680.0\n",
            "3696.0\n",
            "3712.0\n",
            "3728.0\n",
            "3744.0\n",
            "3760.0\n",
            "3776.0\n",
            "3792.0\n",
            "3808.0\n",
            "3824.0\n",
            "3840.0\n",
            "3856.0\n",
            "3872.0\n",
            "3888.0\n",
            "3904.0\n",
            "3920.0\n",
            "3936.0\n",
            "3952.0\n",
            "3968.0\n",
            "3984.0\n",
            "4000.0\n",
            "4016.0\n",
            "4032.0\n",
            "4048.0\n",
            "4064.0\n",
            "4080.0\n",
            "4096.0\n",
            "4112.0\n",
            "4128.0\n",
            "4144.0\n",
            "4160.0\n",
            "4176.0\n",
            "4192.0\n",
            "4208.0\n",
            "4224.0\n",
            "4240.0\n",
            "4256.0\n",
            "4272.0\n",
            "4277.0\n",
            "Mean: tensor([0.4331, 0.4035, 0.3942])\n",
            "Standard Deviation: tensor([0.1785, 0.1713, 0.1607])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a custom class to create a dataset. Since the images in the training folder are defined by class, label, and state, but the images in the testing folder aren't classified in any way, I just decided to make two separate classes for the test_data and the train_data."
      ],
      "metadata": {
        "id": "6kLvFOS773RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.file_list = [f for f in os.listdir(root_dir) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.file_list[idx])\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name  # Return image and file name"
      ],
      "metadata": {
        "id": "CiYg-84P3k9k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the datasets and transform them so some of them are flipped (for additional training). I used the values from the means and standard deviations found in the previous step to normalize."
      ],
      "metadata": {
        "id": "r-9yo6rCHYXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training dataset\n",
        "def create_datasets(batch_size):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.Resize((1024, 1024)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.43303847, 0.4034577, 0.39415097], [0.18344551, 0.17549995, 0.1647388])\n",
        "    ])\n",
        "\n",
        "    # Choose the training datasets\n",
        "    train_data = datasets.ImageFolder(os.path.join('/content/drive/MyDrive', 'train_folder'), train_transform)\n",
        "\n",
        "    # Define validation set size\n",
        "    valid_size = 0.95 #Ideally would be somewhere closer to 0.2, but the training data took a very long time to go through even at 0.9.\n",
        "\n",
        "    # Get indices for training and validation sets\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "    # Define samplers for training and validation batches\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # Load training data in batches\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
        "\n",
        "    # Load validation data in batches\n",
        "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=0)\n",
        "\n",
        "    return train_data, train_loader, valid_loader"
      ],
      "metadata": {
        "id": "qKvw4rla7Ah5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where I defined the rest of the variables for the testing data, including the test_transform, test_dataset, and test_loader."
      ],
      "metadata": {
        "id": "7gZ0ZQp98Dim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_loader, valid_loader = create_datasets(batch_size=4)\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize((1024, 1024)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.43303847, 0.4034577, 0.39415097], [0.18344551, 0.17549995, 0.1647388])\n",
        "    ])\n",
        "test_dataset = CustomTestDataset(root_dir = '/content/drive/MyDrive/test', transform = test_transform)\n",
        "print(f'There are {len(test_dataset)} images in the test dataset.')\n",
        "test_loader = DataLoader(test_dataset, batch_size = 4, shuffle = False, num_workers = 0) #When actually testing, batch_size = len(test_dataset)\n",
        "print(f'There are {len(test_loader)} images in the test loader.')\n",
        "\n",
        "class_names = train_data.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfVDl26Qn8pw",
        "outputId": "1a9aca81-7da0-4de9-c8a1-36460ff154d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2154 images in the test dataset.\n",
            "There are 539 images in the test loader.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple version of a Convolutional Neural Network with (Conv layer, ReLU layer, a pooling layer) x 2, then two fully conneced linear layers. This is also where a lot of the tweaks to the module itself will be made, including adding more layers or changing the transformations."
      ],
      "metadata": {
        "id": "48j8Q1haHioL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 5) #Conv layer\n",
        "        self.pool = nn.MaxPool2d(2, 2) #Pooling layer\n",
        "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 253 * 253, 150) #Fully connected linear layer\n",
        "        self.fc2 = nn.Linear(150, len(label_list))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x))) #Conv layer, relu, pooling layer in that order\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "D4dMav1o7Olx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the model, loss function as Cross Entropy, optimizer as Adam with learning rate = 1e-3. We could also try to change the learning rate."
      ],
      "metadata": {
        "id": "FUbG-uOLH5Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "HOOrEWbT77Rq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the training loop. The num_epochs can be changed as needed to see its efficiency and accuracy. The trade-off is time."
      ],
      "metadata": {
        "id": "kFWODdwxICAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5  # Change the number of epochs as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    samplex = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        samplex += 1\n",
        "        running_loss += loss.item()\n",
        "        print(f'{running_loss}, {samplex}')\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPw5QT0o8Buq",
        "outputId": "0373a3e2-6655-449d-c677-ac44c3c77ac8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.440212726593018, 1.0\n",
            "186.45623445510864, 2.0\n",
            "297.30325746536255, 3.0\n",
            "484.4775586128235, 4.0\n",
            "501.40160608291626, 5.0\n",
            "544.8854851722717, 6.0\n",
            "591.0669350624084, 7.0\n",
            "598.820972442627, 8.0\n",
            "632.6691284179688, 9.0\n",
            "640.7030277252197, 10.0\n",
            "646.7003192901611, 11.0\n",
            "650.980143070221, 12.0\n",
            "654.1828672885895, 13.0\n",
            "660.4033615589142, 14.0\n",
            "663.0982055664062, 15.0\n",
            "668.9616208076477, 16.0\n",
            "671.3438699245453, 17.0\n",
            "676.5627520084381, 18.0\n",
            "681.8029110431671, 19.0\n",
            "691.2654592990875, 20.0\n",
            "695.9270842075348, 21.0\n",
            "698.5564460754395, 22.0\n",
            "703.5745644569397, 23.0\n",
            "707.4826436042786, 24.0\n",
            "711.542414188385, 25.0\n",
            "713.675684928894, 26.0\n",
            "716.4751303195953, 27.0\n",
            "721.2222306728363, 28.0\n",
            "724.7413501739502, 29.0\n",
            "729.39479637146, 30.0\n",
            "733.0216174125671, 31.0\n",
            "736.0111453533173, 32.0\n",
            "739.6233327388763, 33.0\n",
            "742.9189586639404, 34.0\n",
            "746.90824842453, 35.0\n",
            "750.1590332984924, 36.0\n",
            "753.3483834266663, 37.0\n",
            "756.0788683891296, 38.0\n",
            "760.7672362327576, 39.0\n",
            "763.7989320755005, 40.0\n",
            "767.4403605461121, 41.0\n",
            "770.4639623165131, 42.0\n",
            "772.8771233558655, 43.0\n",
            "776.9750061035156, 44.0\n",
            "780.8415315151215, 45.0\n",
            "781.8419897556305, 46.0\n",
            "784.2936387062073, 47.0\n",
            "787.9091746807098, 48.0\n",
            "789.831848025322, 49.0\n",
            "792.3559039831161, 50.0\n",
            "795.0522476434708, 51.0\n",
            "802.0959497690201, 52.0\n",
            "803.7527614831924, 53.0\n",
            "804.5551775097847, 54.0\n",
            "Epoch 1/5, Loss: 14.899169953884902\n",
            "0.6966590881347656, 1.0\n",
            "3.4315578937530518, 2.0\n",
            "6.423126697540283, 3.0\n",
            "8.168827533721924, 4.0\n",
            "11.145658493041992, 5.0\n",
            "13.659754276275635, 6.0\n",
            "17.109008312225342, 7.0\n",
            "19.183332443237305, 8.0\n",
            "21.06721305847168, 9.0\n",
            "23.547515869140625, 10.0\n",
            "27.644225120544434, 11.0\n",
            "29.431198954582214, 12.0\n",
            "32.74439513683319, 13.0\n",
            "33.97718131542206, 14.0\n",
            "36.471192479133606, 15.0\n",
            "39.0373615026474, 16.0\n",
            "42.012126326560974, 17.0\n",
            "44.49631321430206, 18.0\n",
            "46.25395178794861, 19.0\n",
            "47.424012899398804, 20.0\n",
            "48.71950590610504, 21.0\n",
            "50.10047698020935, 22.0\n",
            "51.943867206573486, 23.0\n",
            "55.80585479736328, 24.0\n",
            "57.52676856517792, 25.0\n",
            "59.660784125328064, 26.0\n",
            "61.148847222328186, 27.0\n",
            "61.741803884506226, 28.0\n",
            "63.18767523765564, 29.0\n",
            "65.02152359485626, 30.0\n",
            "65.76678550243378, 31.0\n",
            "66.8427654504776, 32.0\n",
            "69.17723953723907, 33.0\n",
            "77.16637623310089, 34.0\n",
            "80.40861713886261, 35.0\n",
            "80.83040761947632, 36.0\n",
            "82.13845324516296, 37.0\n",
            "86.52144837379456, 38.0\n",
            "86.86827909946442, 39.0\n",
            "89.07889711856842, 40.0\n",
            "90.73271930217743, 41.0\n",
            "91.83058440685272, 42.0\n",
            "93.3900054693222, 43.0\n",
            "95.6504784822464, 44.0\n",
            "98.08122932910919, 45.0\n",
            "100.97970712184906, 46.0\n",
            "103.41452705860138, 47.0\n",
            "104.37398052215576, 48.0\n",
            "105.1387368440628, 49.0\n",
            "106.85826849937439, 50.0\n",
            "109.5018150806427, 51.0\n",
            "112.06298828125, 52.0\n",
            "115.80502319335938, 53.0\n",
            "118.03944253921509, 54.0\n",
            "Epoch 2/5, Loss: 2.1859156025780573\n",
            "0.3726552724838257, 1.0\n",
            "2.9684542417526245, 2.0\n",
            "4.10328209400177, 3.0\n",
            "4.383287042379379, 4.0\n",
            "5.449307173490524, 5.0\n",
            "6.165467947721481, 6.0\n",
            "8.240726202726364, 7.0\n",
            "9.890275329351425, 8.0\n",
            "11.638759702444077, 9.0\n",
            "14.504279226064682, 10.0\n",
            "17.190327495336533, 11.0\n",
            "18.922077387571335, 12.0\n",
            "19.78228470683098, 13.0\n",
            "21.319005221128464, 14.0\n",
            "23.026866048574448, 15.0\n",
            "25.6025148332119, 16.0\n",
            "28.505295366048813, 17.0\n",
            "30.34776458144188, 18.0\n",
            "32.8190995156765, 19.0\n",
            "36.328359454870224, 20.0\n",
            "38.12307605147362, 21.0\n",
            "39.93212339282036, 22.0\n",
            "40.59713026881218, 23.0\n",
            "41.05671384930611, 24.0\n",
            "42.67986598610878, 25.0\n",
            "42.95509830117226, 26.0\n",
            "45.900106340646744, 27.0\n",
            "49.49517074227333, 28.0\n",
            "52.07016387581825, 29.0\n",
            "54.08792391419411, 30.0\n",
            "54.35772827267647, 31.0\n",
            "55.10454073548317, 32.0\n",
            "56.82019999623299, 33.0\n",
            "60.88534453511238, 34.0\n",
            "62.68803170323372, 35.0\n",
            "63.980407029390335, 36.0\n",
            "64.9763802587986, 37.0\n",
            "66.4281977713108, 38.0\n",
            "67.31077966094017, 39.0\n",
            "68.83045360445976, 40.0\n",
            "69.5947554409504, 41.0\n",
            "69.85201779007912, 42.0\n",
            "71.81356301903725, 43.0\n",
            "74.59254184365273, 44.0\n",
            "77.06371155381203, 45.0\n",
            "78.44995120167732, 46.0\n",
            "82.06651929020882, 47.0\n",
            "82.31794556975365, 48.0\n",
            "85.50140437483788, 49.0\n",
            "86.91213056445122, 50.0\n",
            "87.78625378012657, 51.0\n",
            "87.98951211571693, 52.0\n",
            "88.77033457159996, 53.0\n",
            "90.2488497197628, 54.0\n",
            "Epoch 3/5, Loss: 1.6712749948104222\n",
            "0.9453436136245728, 1.0\n",
            "1.2069363594055176, 2.0\n",
            "1.6214683055877686, 3.0\n",
            "1.8841505944728851, 4.0\n",
            "5.906840890645981, 5.0\n",
            "7.19281205534935, 6.0\n",
            "9.386742442846298, 7.0\n",
            "10.197709828615189, 8.0\n",
            "13.367365151643753, 9.0\n",
            "14.359079033136368, 10.0\n",
            "15.270345896482468, 11.0\n",
            "15.816929131746292, 12.0\n",
            "16.899123460054398, 13.0\n",
            "18.02688243985176, 14.0\n",
            "20.17316034436226, 15.0\n",
            "20.84215274453163, 16.0\n",
            "23.231224209070206, 17.0\n",
            "25.278081566095352, 18.0\n",
            "26.05471220612526, 19.0\n",
            "26.600387066602707, 20.0\n",
            "26.960186511278152, 21.0\n",
            "27.044466719031334, 22.0\n",
            "27.71086297929287, 23.0\n",
            "29.72662816941738, 24.0\n",
            "30.15664492547512, 25.0\n",
            "30.32151736319065, 26.0\n",
            "31.297779574990273, 27.0\n",
            "32.31039918959141, 28.0\n",
            "32.531853184103966, 29.0\n",
            "32.58641254156828, 30.0\n",
            "32.68490021675825, 31.0\n",
            "37.412149362266064, 32.0\n",
            "38.326597563922405, 33.0\n",
            "42.30966817587614, 34.0\n",
            "43.03173600882292, 35.0\n",
            "43.84618370980024, 36.0\n",
            "45.593518666923046, 37.0\n",
            "46.62878566235304, 38.0\n",
            "47.13535260409117, 39.0\n",
            "48.591331116855145, 40.0\n",
            "50.589078299701214, 41.0\n",
            "52.91601072996855, 42.0\n",
            "53.33032917231321, 43.0\n",
            "53.93131374567747, 44.0\n",
            "56.0380599424243, 45.0\n",
            "56.522565834224224, 46.0\n",
            "58.63109540194273, 47.0\n",
            "60.78767680376768, 48.0\n",
            "63.08692120760679, 49.0\n",
            "66.16623234003782, 50.0\n",
            "68.48532032221556, 51.0\n",
            "70.58211826533079, 52.0\n",
            "72.01146947592497, 53.0\n",
            "72.94065784662962, 54.0\n",
            "Epoch 4/5, Loss: 1.3507529230857338\n",
            "1.2469878196716309, 1.0\n",
            "4.393882513046265, 2.0\n",
            "4.835775226354599, 3.0\n",
            "5.360381215810776, 4.0\n",
            "5.480960950255394, 5.0\n",
            "8.067917212843895, 6.0\n",
            "10.355497941374779, 7.0\n",
            "11.668514594435692, 8.0\n",
            "13.809848174452782, 9.0\n",
            "14.838812932372093, 10.0\n",
            "16.57279632985592, 11.0\n",
            "18.294494971632957, 12.0\n",
            "19.611906990408897, 13.0\n",
            "20.225183233618736, 14.0\n",
            "22.95777676999569, 15.0\n",
            "25.50651691854, 16.0\n",
            "26.304733976721764, 17.0\n",
            "27.58113096654415, 18.0\n",
            "28.310933753848076, 19.0\n",
            "30.089857026934624, 20.0\n",
            "31.42834274470806, 21.0\n",
            "32.53716878592968, 22.0\n",
            "33.07494978606701, 23.0\n",
            "33.18734174966812, 24.0\n",
            "33.625040501356125, 25.0\n",
            "33.864173501729965, 26.0\n",
            "34.00208982825279, 27.0\n",
            "34.5955893099308, 28.0\n",
            "35.47564688324928, 29.0\n",
            "35.86542996764183, 30.0\n",
            "36.640656381845474, 31.0\n",
            "37.457585006952286, 32.0\n",
            "38.87930205464363, 33.0\n",
            "39.040243566036224, 34.0\n",
            "41.821504056453705, 35.0\n",
            "45.46782797574997, 36.0\n",
            "46.12298846244812, 37.0\n",
            "47.68765604496002, 38.0\n",
            "50.28560793399811, 39.0\n",
            "51.83010458946228, 40.0\n",
            "51.86330385878682, 41.0\n",
            "52.288622219115496, 42.0\n",
            "54.20196397975087, 43.0\n",
            "54.809893149882555, 44.0\n",
            "55.24577825143933, 45.0\n",
            "55.29194188117981, 46.0\n",
            "56.15110194683075, 47.0\n",
            "56.989212810993195, 48.0\n",
            "57.40778601169586, 49.0\n",
            "57.57262232899666, 50.0\n",
            "57.610896334052086, 51.0\n",
            "58.820189103484154, 52.0\n",
            "59.96307896077633, 53.0\n",
            "63.104885801672935, 54.0\n",
            "Epoch 5/5, Loss: 1.1686089963272766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These labels are the ones that are \"good\" (no anomalies)"
      ],
      "metadata": {
        "id": "ir9a7hUnIGV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_label = ['bottle-good', 'cable-good', 'capsule-good', 'carpet-good', 'grid-good', 'hazelnut-good', 'leather-good', 'metal_nut-good', 'pill-good', 'screw-good',\n",
        "                 'tile-good', 'toothbrush-good', 'transistor-good', 'wood-good', 'zipper-good']\n",
        "def is_good(label):\n",
        "  return label in correct_label\n",
        "good_bad_map = {i: 1 if is_good(class_name) else 0 for i, class_name in enumerate(class_names)}"
      ],
      "metadata": {
        "id": "-z6CCIBXD3XW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This checks if the predictions equal the actual label in two parts: first the actual label, then whether it's in the correct pile of \"good\" or not. The number of batches can also be changed and ideally shouldn't even exist, but I found running the model with the entire training dataset took an insane amount of time, so I took a small percentage of the data."
      ],
      "metadata": {
        "id": "qSCHWpX1IaxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_running_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "correct_good_bad = 0\n",
        "total_good_bad = 0\n",
        "\n",
        "num_batches_to_check = 4 # Ideally, this would not even be necessary.\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, labels) in enumerate(valid_loader):\n",
        "        if batch_idx >= num_batches_to_check: # Would delete this if statement if faster.\n",
        "            break\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        predicted_good_bad = torch.tensor([good_bad_map[pred.item()] for pred in predicted], device=device)\n",
        "        labels_good_bad = torch.tensor([good_bad_map[label.item()] for label in labels], device=device)\n",
        "\n",
        "        print(predicted_good_bad)\n",
        "        print(labels_good_bad)\n",
        "\n",
        "        total_good_bad += labels_good_bad.size(0)\n",
        "        correct_good_bad += (predicted_good_bad == labels_good_bad).sum().item()\n",
        "\n",
        "        print(total)\n",
        "\n",
        "    val_loss = val_running_loss/len(valid_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    val_good_bad_accuracy = 100 * correct_good_bad / total_good_bad\n",
        "\n",
        "    print(f'{correct}, {total}')\n",
        "    print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}%\")\n",
        "    print(f\"Good/Bad Accuracy: {val_good_bad_accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sIn_GkJPr7p",
        "outputId": "7a2af330-f56c-4d39-a544-f3dc0068ccf1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "4\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 0], device='cuda:0')\n",
            "8\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([0, 0, 0, 1], device='cuda:0')\n",
            "12\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1], device='cuda:0')\n",
            "16\n",
            "11, 16\n",
            "Validation Loss: 0.010999997505756814, Validation Accuracy: 68.75%\n",
            "Good/Bad Accuracy: 68.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is now using the testing data. it uses the model to estimate what each new image will be. The dataset was again significantly reduced for time efficiency purposes."
      ],
      "metadata": {
        "id": "HSVNA4ri8NPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "num_batches_to_check_test = 4 # Ideally would not be necessary\n",
        "with torch.no_grad():\n",
        "    for idx, (inputs, labels) in enumerate(test_loader):\n",
        "        if idx >= num_batches_to_check_test: # Would delete if faster\n",
        "            break\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        for i in range(len(predicted)):\n",
        "            predictions.append((idx * test_loader.batch_size + i, class_names[predicted[i].item()]))\n",
        "        print(f'Processed batch {idx + 1} / {len(test_loader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv7poZaCRFhI",
        "outputId": "c176c62b-fecc-4cf0-b70f-3e7a626f607a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed batch 1 / 539\n",
            "Processed batch 2 / 539\n",
            "Processed batch 3 / 539\n",
            "Processed batch 4 / 539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will move the data into a csv called \"test_predictions.csv\" and download it to files from the local google.colab."
      ],
      "metadata": {
        "id": "4pdLniuT8dz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write predictions to CSV\n",
        "pred_df = pd.DataFrame(predictions, columns=['Index', 'Prediction'])\n",
        "pred_df.to_csv('test_predictions.csv', index=False)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download('test_predictions.csv')"
      ],
      "metadata": {
        "id": "xbXkFELj3S1P"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}